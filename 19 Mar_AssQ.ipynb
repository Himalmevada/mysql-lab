{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 : What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a common data preprocessing technique used to transform numeric features or variables in a dataset to a specific range, typically between 0 and 1. It involves scaling the data based on the minimum and maximum values present in the feature, using the following formula:\n",
    "\n",
    "x' = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "where x is the original value, x' is the scaled value, min(x) is the minimum value in the feature, and max(x) is the maximum value in the feature.\n",
    "\n",
    "The purpose of Min-Max scaling is to bring all the features onto the same scale, eliminating the differences in the ranges and preventing any particular feature from dominating the learning algorithm or model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29157939],\n",
       "       [0.1522832 ],\n",
       "       [0.3757855 ],\n",
       "       [0.43171345],\n",
       "       [0.45077503],\n",
       "       [0.46543779],\n",
       "       [0.11939673],\n",
       "       [0.49874319],\n",
       "       [0.25073314],\n",
       "       [0.24528697],\n",
       "       [0.15081693],\n",
       "       [0.67427734],\n",
       "       [0.25869292],\n",
       "       [0.32174277],\n",
       "       [0.24633431],\n",
       "       [0.38772518],\n",
       "       [0.15207373],\n",
       "       [0.27691663],\n",
       "       [0.29116045],\n",
       "       [0.36824466],\n",
       "       [0.31105991],\n",
       "       [0.36070381],\n",
       "       [0.2660243 ],\n",
       "       [0.761416  ],\n",
       "       [0.35085882],\n",
       "       [0.30875576],\n",
       "       [0.21575199],\n",
       "       [0.20150817],\n",
       "       [0.39023879],\n",
       "       [0.34729786],\n",
       "       [0.13573523],\n",
       "       [0.32006703],\n",
       "       [0.25115207],\n",
       "       [0.36908253],\n",
       "       [0.30812736],\n",
       "       [0.43967323],\n",
       "       [0.27733557],\n",
       "       [0.29032258],\n",
       "       [0.32718894],\n",
       "       [0.59069962],\n",
       "       [0.27167993],\n",
       "       [0.30142438],\n",
       "       [0.22769166],\n",
       "       [0.13845832],\n",
       "       [0.57247591],\n",
       "       [0.31881022],\n",
       "       [0.40134059],\n",
       "       [0.6143695 ],\n",
       "       [0.53372434],\n",
       "       [0.31357352],\n",
       "       [0.19836615],\n",
       "       [0.15123586],\n",
       "       [0.66485128],\n",
       "       [0.14390448],\n",
       "       [0.47109342],\n",
       "       [0.34394638],\n",
       "       [0.73188102],\n",
       "       [0.4888982 ],\n",
       "       [0.17113532],\n",
       "       [0.94679514],\n",
       "       [0.36070381],\n",
       "       [0.22496858],\n",
       "       [0.16652702],\n",
       "       [0.31881022],\n",
       "       [0.30414747],\n",
       "       [0.35630499],\n",
       "       [0.28026812],\n",
       "       [0.        ],\n",
       "       [0.359447  ],\n",
       "       [0.25010473],\n",
       "       [0.18747382],\n",
       "       [0.29325513],\n",
       "       [0.49832426],\n",
       "       [0.46522832],\n",
       "       [0.24423963],\n",
       "       [0.15584416],\n",
       "       [0.31105991],\n",
       "       [0.50544617],\n",
       "       [0.4124424 ],\n",
       "       [0.29786343],\n",
       "       [0.34289904],\n",
       "       [0.28466695],\n",
       "       [0.14662757],\n",
       "       [0.6202346 ],\n",
       "       [0.27042313],\n",
       "       [0.66527021],\n",
       "       [0.20863008],\n",
       "       [0.31860075],\n",
       "       [0.45328865],\n",
       "       [0.37892752],\n",
       "       [0.54252199],\n",
       "       [0.40678676],\n",
       "       [0.05613741],\n",
       "       [0.27754504],\n",
       "       [0.41223293],\n",
       "       [0.7771261 ],\n",
       "       [0.50712191],\n",
       "       [0.18768328],\n",
       "       [0.3757855 ],\n",
       "       [0.19669041],\n",
       "       [0.17343946],\n",
       "       [0.25785505],\n",
       "       [0.86363636],\n",
       "       [0.40532049],\n",
       "       [0.37390029],\n",
       "       [0.25743611],\n",
       "       [0.36489317],\n",
       "       [0.46376204],\n",
       "       [0.31776288],\n",
       "       [0.23544198],\n",
       "       [0.22894847],\n",
       "       [0.0875576 ],\n",
       "       [0.73313783],\n",
       "       [0.43736908],\n",
       "       [0.47423544],\n",
       "       [0.29828236],\n",
       "       [0.56263092],\n",
       "       [0.15877671],\n",
       "       [0.196062  ],\n",
       "       [0.44009217],\n",
       "       [0.18056137],\n",
       "       [0.21679933],\n",
       "       [0.23439464],\n",
       "       [0.26979472],\n",
       "       [0.19710934],\n",
       "       [0.55990783],\n",
       "       [0.11416003],\n",
       "       [0.2398408 ],\n",
       "       [0.17406787],\n",
       "       [0.4136992 ],\n",
       "       [0.33535819],\n",
       "       [0.36028488],\n",
       "       [0.16966904],\n",
       "       [0.19250105],\n",
       "       [0.31818182],\n",
       "       [0.11395057],\n",
       "       [0.15207373],\n",
       "       [0.23209049],\n",
       "       [0.27084206],\n",
       "       [0.21135316],\n",
       "       [0.30163385],\n",
       "       [0.65416841],\n",
       "       [0.79849183],\n",
       "       [0.50230415],\n",
       "       [0.27984918],\n",
       "       [0.11059908],\n",
       "       [0.3261416 ],\n",
       "       [0.1843318 ],\n",
       "       [0.140553  ],\n",
       "       [0.09300377],\n",
       "       [0.23041475],\n",
       "       [0.21072476],\n",
       "       [0.29723502],\n",
       "       [0.44993716],\n",
       "       [0.34981148],\n",
       "       [0.56095517],\n",
       "       [0.94470046],\n",
       "       [0.45936322],\n",
       "       [0.21617093],\n",
       "       [0.28110599],\n",
       "       [0.38604943],\n",
       "       [0.20087977],\n",
       "       [0.27524089],\n",
       "       [0.22496858],\n",
       "       [0.30247172],\n",
       "       [0.44930876],\n",
       "       [0.37054881],\n",
       "       [0.59991621],\n",
       "       [0.1575199 ],\n",
       "       [0.15835777],\n",
       "       [1.        ],\n",
       "       [0.26686217],\n",
       "       [0.0875576 ],\n",
       "       [0.60284876],\n",
       "       [0.28801843],\n",
       "       [0.6248429 ],\n",
       "       [0.3104315 ],\n",
       "       [0.23900293],\n",
       "       [0.13678257],\n",
       "       [0.66108085],\n",
       "       [0.66149979],\n",
       "       [0.42438207],\n",
       "       [0.8856305 ],\n",
       "       [0.42103058],\n",
       "       [0.78508588],\n",
       "       [0.36908253],\n",
       "       [0.37348136],\n",
       "       [0.57373272],\n",
       "       [0.31587767],\n",
       "       [0.41956431],\n",
       "       [0.26434855],\n",
       "       [0.35064935],\n",
       "       [0.53142019],\n",
       "       [0.25994973],\n",
       "       [0.2829912 ],\n",
       "       [0.09405111],\n",
       "       [0.1522832 ],\n",
       "       [0.83870968],\n",
       "       [0.20800168],\n",
       "       [0.21868454],\n",
       "       [0.32760788],\n",
       "       [0.20255551],\n",
       "       [0.20800168],\n",
       "       [0.27922078],\n",
       "       [0.36573104],\n",
       "       [0.28068705],\n",
       "       [0.49266862],\n",
       "       [0.74696271],\n",
       "       [0.44407206],\n",
       "       [0.20297444],\n",
       "       [0.565354  ],\n",
       "       [0.47800587],\n",
       "       [0.94805195],\n",
       "       [0.21365731],\n",
       "       [0.52576456],\n",
       "       [0.205907  ],\n",
       "       [0.52534562],\n",
       "       [0.17846669],\n",
       "       [0.09782153],\n",
       "       [0.56702974],\n",
       "       [0.19040637],\n",
       "       [0.21679933],\n",
       "       [0.11541684],\n",
       "       [0.27042313],\n",
       "       [0.21679933],\n",
       "       [0.2764977 ],\n",
       "       [0.1470465 ],\n",
       "       [0.3640553 ],\n",
       "       [0.21386678],\n",
       "       [0.39903645],\n",
       "       [0.43862589],\n",
       "       [0.26434855],\n",
       "       [0.17888563],\n",
       "       [0.16129032],\n",
       "       [0.26099707],\n",
       "       [0.14662757],\n",
       "       [0.19962296],\n",
       "       [0.62337662],\n",
       "       [0.68621701],\n",
       "       [0.5437788 ],\n",
       "       [0.50502723],\n",
       "       [0.41055718],\n",
       "       [0.30896523],\n",
       "       [0.32907415]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = sns.load_dataset(\"tips\")\n",
    "minmax = MinMaxScaler()\n",
    "minmax.fit(df[[\"total_bill\"]])\n",
    "minmax.transform(df[[\"total_bill\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 : What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization or feature scaling, is a method used to transform feature vectors into a unit vector or a vector with a length of 1. Unlike Min-Max scaling, which scales the features to a specific range (e.g., 0 to 1), the Unit Vector technique focuses on preserving the direction and relative relationships among the features.\n",
    "\n",
    "The process of transforming a feature vector into a unit vector involves dividing each element of the vector by its Euclidean norm, which is the square root of the sum of squares of its elements. The formula for calculating the unit vector is as follows:\n",
    "\n",
    "v' = v / ||v||\n",
    "\n",
    "where v is the original feature vector, v' is the unit vector, and ||v|| denotes the Euclidean norm of v.\n",
    "\n",
    "The Unit Vector technique ensures that each feature in the vector has the same scale, allowing them to contribute equally during computations or analyses. It is particularly useful when the magnitude or absolute values of the features are not as important as their direction or relative relationships.\n",
    "\n",
    "To summarize, the key differences between Min-Max scaling and the Unit Vector technique are as follows:\n",
    "\n",
    "1. Range: Min-Max scaling scales features to a specific range (e.g., 0 to 1), while the Unit Vector technique transforms feature vectors into unit vectors with a length of 1.\n",
    "\n",
    "2. Preservation: Min-Max scaling alters the values of the features, while the Unit Vector technique preserves the direction and relative relationships among the features.\n",
    "\n",
    "3. Application: Min-Max scaling is commonly used when the absolute values or magnitudes of the features are important. The Unit Vector technique is more suitable when the direction or relative relationships among the features are significant.\n",
    "\n",
    "In practice, the choice between Min-Max scaling and the Unit Vector technique depends on the specific requirements of the problem and the nature of the features being scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99823771, 0.05934197],\n",
       "       [0.98735707, 0.15851187],\n",
       "       [0.98640661, 0.16432285],\n",
       "       [0.99037159, 0.13843454],\n",
       "       [0.98939488, 0.14525073],\n",
       "       [0.98309589, 0.18309141],\n",
       "       [0.97496878, 0.2223418 ],\n",
       "       [0.99333102, 0.11529735],\n",
       "       [0.99161511, 0.12922644],\n",
       "       [0.97694312, 0.21349975],\n",
       "       [0.98641987, 0.16424323],\n",
       "       [0.99009498, 0.14039917],\n",
       "       [0.99485672, 0.10129216],\n",
       "       [0.98700924, 0.16066347],\n",
       "       [0.97988851, 0.19954574],\n",
       "       [0.98389908, 0.17872495],\n",
       "       [0.9871829 , 0.15959298],\n",
       "       [0.9750328 , 0.22206088],\n",
       "       [0.97938658, 0.20199487],\n",
       "       [0.98709527, 0.1601341 ],\n",
       "       [0.97504726, 0.22199737],\n",
       "       [0.9909398 , 0.13430677],\n",
       "       [0.99014941, 0.14001479],\n",
       "       [0.98201   , 0.18882891],\n",
       "       [0.98737215, 0.15841793],\n",
       "       [0.99147891, 0.1302673 ],\n",
       "       [0.98899596, 0.14794255],\n",
       "       [0.98780711, 0.15568276],\n",
       "       [0.98092686, 0.19437721],\n",
       "       [0.98854552, 0.15092298],\n",
       "       [0.98866899, 0.15011205],\n",
       "       [0.99084659, 0.13499272],\n",
       "       [0.98073067, 0.19536467],\n",
       "       [0.99306186, 0.11759312],\n",
       "       [0.98350502, 0.18088084],\n",
       "       [0.98899056, 0.14797864],\n",
       "       [0.9925654 , 0.1217125 ],\n",
       "       [0.98395349, 0.17842512],\n",
       "       [0.99244848, 0.12266217],\n",
       "       [0.98745639, 0.15789197],\n",
       "       [0.99038917, 0.13830871],\n",
       "       [0.9895835 , 0.14396003],\n",
       "       [0.97674434, 0.21440729],\n",
       "       [0.99083017, 0.1351132 ],\n",
       "       [0.98345319, 0.18116243],\n",
       "       [0.98681354, 0.16186116],\n",
       "       [0.9756262 , 0.21943909],\n",
       "       [0.983282  , 0.18208926],\n",
       "       [0.99743203, 0.07161946],\n",
       "       [0.98645298, 0.16404429],\n",
       "       [0.98070081, 0.19551452],\n",
       "       [0.96952977, 0.24497351],\n",
       "       [0.98902579, 0.14774301],\n",
       "       [0.98790759, 0.15504385],\n",
       "       [0.98588897, 0.16740055],\n",
       "       [0.98416747, 0.17724104],\n",
       "       [0.99689977, 0.07868191],\n",
       "       [0.99839096, 0.05670528],\n",
       "       [0.98796171, 0.15469863],\n",
       "       [0.99041991, 0.13808838],\n",
       "       [0.98771556, 0.15626254],\n",
       "       [0.98967534, 0.14332735],\n",
       "       [0.98423933, 0.17684155],\n",
       "       [0.97951611, 0.20136581],\n",
       "       [0.98892398, 0.14842293],\n",
       "       [0.98791805, 0.15497718],\n",
       "       [0.98891429, 0.14848744],\n",
       "       [0.95082902, 0.30971629],\n",
       "       [0.9951003 , 0.09887057],\n",
       "       [0.99044476, 0.13791003],\n",
       "       [0.98683411, 0.16173571],\n",
       "       [0.9849053 , 0.17309408],\n",
       "       [0.99323616, 0.11611175],\n",
       "       [0.98099635, 0.19402618],\n",
       "       [0.98902973, 0.14771659],\n",
       "       [0.99300147, 0.11810198],\n",
       "       [0.98554889, 0.16939122],\n",
       "       [0.98935914, 0.14549399],\n",
       "       [0.99142462, 0.13067987],\n",
       "       [0.98793839, 0.15484749],\n",
       "       [0.98830106, 0.1525156 ],\n",
       "       [0.97980406, 0.19996001],\n",
       "       [0.98388554, 0.17879946],\n",
       "       [0.98849725, 0.15123887],\n",
       "       [0.99202757, 0.12602102],\n",
       "       [0.98916225, 0.14682655],\n",
       "       [0.98842426, 0.15171516],\n",
       "       [0.97688607, 0.21376063],\n",
       "       [0.9731012 , 0.23037807],\n",
       "       [0.99009867, 0.14037316],\n",
       "       [0.99468088, 0.10300458],\n",
       "       [0.98810609, 0.15377374],\n",
       "       [0.98521175, 0.17134117],\n",
       "       [0.96699774, 0.25478494],\n",
       "       [0.98994949, 0.14142136],\n",
       "       [0.99313879, 0.11694166],\n",
       "       [0.98942047, 0.14507631],\n",
       "       [0.9923159 , 0.12373016],\n",
       "       [0.98995892, 0.14135539],\n",
       "       [0.99283152, 0.11952225],\n",
       "       [0.97659027, 0.21510799],\n",
       "       [0.98150229, 0.19145038],\n",
       "       [0.99841143, 0.05634376],\n",
       "       [0.98816699, 0.15338185],\n",
       "       [0.9815078 , 0.19142217],\n",
       "       [0.99434827, 0.10616739],\n",
       "       [0.98092896, 0.1943666 ],\n",
       "       [0.98582805, 0.16775892],\n",
       "       [0.97940711, 0.20189532],\n",
       "       [0.96308275, 0.26920552],\n",
       "       [0.97780241, 0.20952909],\n",
       "       [0.99062113, 0.1366374 ],\n",
       "       [0.99452547, 0.1044944 ],\n",
       "       [0.99437962, 0.1058734 ],\n",
       "       [0.98811258, 0.15373202],\n",
       "       [0.9801647 , 0.19818466],\n",
       "       [0.98595419, 0.16701596],\n",
       "       [0.99022651, 0.13946852],\n",
       "       [0.98967697, 0.14331605],\n",
       "       [0.99272781, 0.12038062],\n",
       "       [0.98102995, 0.19385622],\n",
       "       [0.99225511, 0.12421674],\n",
       "       [0.98497764, 0.17268191],\n",
       "       [0.99222995, 0.12441755],\n",
       "       [0.98021649, 0.19792833],\n",
       "       [0.99021357, 0.1395603 ],\n",
       "       [0.98524568, 0.17114596],\n",
       "       [0.99064659, 0.1364527 ],\n",
       "       [0.9849053 , 0.17309408],\n",
       "       [0.99546798, 0.09509729],\n",
       "       [0.99692399, 0.07837453],\n",
       "       [0.99039401, 0.13827405],\n",
       "       [0.99110348, 0.13309357],\n",
       "       [0.98695377, 0.16100388],\n",
       "       [0.98452739, 0.17523078],\n",
       "       [0.98938373, 0.14532664],\n",
       "       [0.98176841, 0.19008101],\n",
       "       [0.99015833, 0.13995171],\n",
       "       [0.99227788, 0.12403473],\n",
       "       [0.97885648, 0.20454828],\n",
       "       [0.98051586, 0.19643993],\n",
       "       [0.98145122, 0.19171205],\n",
       "       [0.99271283, 0.12050411],\n",
       "       [0.98334222, 0.18176381],\n",
       "       [0.9903434 , 0.13863602],\n",
       "       [0.98424492, 0.17681046],\n",
       "       [0.9973489 , 0.07276795],\n",
       "       [0.99070276, 0.13604427],\n",
       "       [0.98471252, 0.17418739],\n",
       "       [0.96632031, 0.25734229],\n",
       "       [0.98457858, 0.17494289],\n",
       "       [0.98859692, 0.15058597],\n",
       "       [0.98763272, 0.15678526],\n",
       "       [0.99669804, 0.0811974 ],\n",
       "       [0.99492193, 0.10064966],\n",
       "       [0.98549631, 0.16969685],\n",
       "       [0.99465602, 0.10324435],\n",
       "       [0.98893635, 0.14834045],\n",
       "       [0.98152754, 0.1913209 ],\n",
       "       [0.99272506, 0.12040328],\n",
       "       [0.98700727, 0.1606756 ],\n",
       "       [0.98105465, 0.19373117],\n",
       "       [0.99247442, 0.12245212],\n",
       "       [0.98967534, 0.14332735],\n",
       "       [0.98563832, 0.16887007],\n",
       "       [0.99007829, 0.14051682],\n",
       "       [0.99422916, 0.10727713],\n",
       "       [0.9900802 , 0.14050334],\n",
       "       [0.98863996, 0.15030315],\n",
       "       [0.98275687, 0.18490251],\n",
       "       [0.98117767, 0.1931072 ],\n",
       "       [0.98060452, 0.19599686],\n",
       "       [0.81525026, 0.57910881],\n",
       "       [0.99505264, 0.09934906],\n",
       "       [0.97286824, 0.23135987],\n",
       "       [0.99556187, 0.09410934],\n",
       "       [0.99380899, 0.11110218],\n",
       "       [0.99059557, 0.13682259],\n",
       "       [0.92307692, 0.38461538],\n",
       "       [0.99478667, 0.10197784],\n",
       "       [0.99440752, 0.10561096],\n",
       "       [0.971905  , 0.23537348],\n",
       "       [0.99703505, 0.07694868],\n",
       "       [0.96283011, 0.27010771],\n",
       "       [0.99727446, 0.07378109],\n",
       "       [0.97201936, 0.23490076],\n",
       "       [0.98626609, 0.16516418],\n",
       "       [0.99785133, 0.0655188 ],\n",
       "       [0.98190985, 0.18934901],\n",
       "       [0.98533672, 0.17062108],\n",
       "       [0.99546119, 0.09516837],\n",
       "       [0.97835547, 0.20693132],\n",
       "       [0.99597319, 0.08965159],\n",
       "       [0.99159327, 0.12939395],\n",
       "       [0.97210987, 0.2345259 ],\n",
       "       [0.98233857, 0.18711211],\n",
       "       [0.9818027 , 0.18990381],\n",
       "       [0.99334115, 0.11521006],\n",
       "       [0.9883717 , 0.15205718],\n",
       "       [0.98921918, 0.14644251],\n",
       "       [0.97790174, 0.20906504],\n",
       "       [0.98778182, 0.15584313],\n",
       "       [0.9883717 , 0.15205718],\n",
       "       [0.98857982, 0.15069814],\n",
       "       [0.98154316, 0.19124075],\n",
       "       [0.98130713, 0.19244821],\n",
       "       [0.99187684, 0.12720196],\n",
       "       [0.99701346, 0.077228  ],\n",
       "       [0.99652023, 0.0833513 ],\n",
       "       [0.98506977, 0.17215561],\n",
       "       [0.99779396, 0.06638682],\n",
       "       [0.98071158, 0.19546048],\n",
       "       [0.98309941, 0.18307252],\n",
       "       [0.98271253, 0.18513801],\n",
       "       [0.97439703, 0.22483424],\n",
       "       [0.99638411, 0.08496299],\n",
       "       [0.99436913, 0.10597184],\n",
       "       [0.99172875, 0.12835143],\n",
       "       [0.98313005, 0.18290792],\n",
       "       [0.99478573, 0.10198699],\n",
       "       [0.98402491, 0.17803082],\n",
       "       [0.96798392, 0.25101222],\n",
       "       [0.97586485, 0.21837535],\n",
       "       [0.98283039, 0.18451134],\n",
       "       [0.9931405 , 0.11692712],\n",
       "       [0.98839977, 0.15187458],\n",
       "       [0.98091575, 0.19443325],\n",
       "       [0.98941028, 0.14514576],\n",
       "       [0.97966223, 0.20065371],\n",
       "       [0.99163038, 0.1291092 ],\n",
       "       [0.99654862, 0.08301113],\n",
       "       [0.98220682, 0.18780245],\n",
       "       [0.95991662, 0.28028573],\n",
       "       [0.99081337, 0.13523636],\n",
       "       [0.98184827, 0.18966805],\n",
       "       [0.99238364, 0.12318566],\n",
       "       [0.99686539, 0.0791163 ],\n",
       "       [0.99936557, 0.03561553],\n",
       "       [0.99161275, 0.12924453],\n",
       "       [0.97983374, 0.19981453],\n",
       "       [0.99730368, 0.07338511],\n",
       "       [0.99613098, 0.08788099],\n",
       "       [0.99521256, 0.09773412],\n",
       "       [0.98747998, 0.15774441]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "normalize(df[[\"total_bill\",\"tip\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 : What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle Component Analysis (PCA) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while preserving the most important information or patterns present in the data. It achieves this by identifying the directions, known as principal components, along which the data varies the most.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "1. Standardize the data: PCA is sensitive to the scale of the features, so it's important to standardize the data by subtracting the mean and dividing by the standard deviation of each feature.\n",
    "\n",
    "2. Compute the covariance matrix: The covariance matrix is calculated based on the standardized data to capture the relationships and variances between the features.\n",
    "\n",
    "3. Calculate the eigenvectors and eigenvalues: The eigenvectors represent the principal components, and the corresponding eigenvalues quantify the amount of variance explained by each principal component.\n",
    "\n",
    "4. Select the principal components: The principal components are ranked based on their corresponding eigenvalues. By selecting a subset of the principal components with the highest eigenvalues, we can retain the most important information while reducing the dimensionality.\n",
    "\n",
    "5. Transform the data: The selected principal components are used to transform the original data into the lower-dimensional space.\n",
    "\n",
    "Here's an example to illustrate the application of PCA:\n",
    "\n",
    "Let's consider a dataset with three features: height, weight, and age, collected from individuals. We want to apply PCA to reduce the dimensionality of the data.\n",
    "\n",
    "Original data:\n",
    "\n",
    "| Height (cm) | Weight (kg) | Age (years) |\n",
    "|-------------|-------------|-------------|\n",
    "| 160         | 50          | 25          |\n",
    "| 175         | 70          | 40          |\n",
    "| 155         | 60          | 35          |\n",
    "| 180         | 80          | 28          |\n",
    "\n",
    "Steps to apply PCA:\n",
    "\n",
    "1. Standardize the data: Subtract the mean and divide by the standard deviation of each feature.\n",
    "\n",
    "2. Compute the covariance matrix:\n",
    "\n",
    "Covariance matrix = [[var(height), cov(height, weight), cov(height, age)],\n",
    "                    [cov(weight, height), var(weight), cov(weight, age)],\n",
    "                    [cov(age, height), cov(age, weight), var(age)]]\n",
    "\n",
    "3. Calculate the eigenvectors and eigenvalues:\n",
    "\n",
    "Eigenvectors = [eigenvector1, eigenvector2, eigenvector3]\n",
    "Eigenvalues = [eigenvalue1, eigenvalue2, eigenvalue3]\n",
    "\n",
    "4. Select the principal components: Rank the eigenvectors based on their corresponding eigenvalues. Choose the top-k eigenvectors to retain the most important information while reducing dimensionality.\n",
    "\n",
    "5. Transform the data: Multiply the original data by the selected eigenvectors to obtain the lower-dimensional representation.\n",
    "\n",
    "The transformed data will have reduced dimensions, with each observation represented by a subset of the most informative principal components.\n",
    "\n",
    "It's important to note that the example provided is a simplified version, and in practice, you would typically work with larger datasets and utilize libraries or software packages to perform PCA, such as scikit-learn in Python or MATLAB's built-in functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 : What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts, as PCA can be used as a feature extraction technique. Feature extraction aims to transform the original features of a dataset into a new set of features that are more informative, representative, or compact. PCA achieves this by identifying the most important patterns or variations in the data and representing them as principal components.\n",
    "\n",
    "In the context of feature extraction using PCA, the steps involved are similar to those described earlier for PCA. However, instead of using PCA to reduce the dimensionality of the data, we use it to extract a reduced set of features that capture the most important information.\n",
    "\n",
    "Here's an example to illustrate the concept of using PCA for feature extraction:\n",
    "\n",
    "Let's consider a dataset of images, where each image is represented by a large number of pixel values. We want to extract a smaller set of features that capture the most important patterns in the images.\n",
    "\n",
    "Original data:\n",
    "\n",
    "| Image  | Pixel 1 | Pixel 2 | Pixel 3 | ... | Pixel n |\n",
    "|--------|---------|---------|---------|-----|---------|\n",
    "| Image1 | 0.8     | 0.2     | 0.5     | ... | 0.9     |\n",
    "| Image2 | 0.3     | 0.6     | 0.1     | ... | 0.4     |\n",
    "| Image3 | 0.7     | 0.4     | 0.6     | ... | 0.2     |\n",
    "| ...    | ...     | ...     | ...     | ... | ...     |\n",
    "\n",
    "Steps to apply PCA for feature extraction:\n",
    "\n",
    "1. Standardize the data: As mentioned before, it is essential to standardize the pixel values to ensure that PCA is not biased by differences in scale.\n",
    "\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix based on the standardized pixel values to capture the relationships and variances between the pixels.\n",
    "\n",
    "3. Calculate the eigenvectors and eigenvalues: Compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues quantify the amount of variance explained by each principal component.\n",
    "\n",
    "4. Select the principal components: Rank the eigenvectors based on their corresponding eigenvalues. Choose the top-k eigenvectors to retain the most important information.\n",
    "\n",
    "5. Transform the data: Multiply the standardized pixel values by the selected eigenvectors to obtain the reduced set of features. Each image is now represented by a smaller set of features derived from the principal components.\n",
    "\n",
    "The resulting feature vectors capture the essential information present in the images while reducing the dimensionality. These new features can then be used for further analysis, such as image classification or clustering.\n",
    "\n",
    "By extracting the most informative features using PCA, we can effectively reduce the dimensionality of the dataset while preserving the most important patterns or variations in the data. This can lead to improved computational efficiency and better performance in machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 : You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, you would follow these steps:\n",
    "\n",
    "Understand the data: Start by familiarizing yourself with the dataset and the specific features it contains. In this case, you mentioned features such as price, rating, and delivery time.\n",
    "\n",
    "Normalize the features: Since the features in the dataset may have different scales, it is important to bring them onto the same scale using Min-Max scaling.\n",
    "\n",
    "Determine the range: Decide on the desired range to which you want to scale the features. Commonly, Min-Max scaling is applied to scale the features between 0 and 1, but depending on the specific requirements and characteristics of the dataset, you can choose a different range.\n",
    "\n",
    "Compute the minimum and maximum values: Calculate the minimum and maximum values for each feature. For example, for the price feature, find the minimum and maximum prices in the dataset.\n",
    "\n",
    "Apply Min-Max scaling: Utilize the formula mentioned earlier to scale the features:\n",
    "\n",
    "Scaled value = (Original value - Minimum value) / (Maximum value - Minimum value)\n",
    "\n",
    "For each feature, apply this formula to obtain the scaled values.\n",
    "\n",
    "Verify the scaled data: After applying Min-Max scaling, ensure that the scaled values for each feature fall within the desired range (e.g., 0 to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 :You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the dimensionality of the dataset for predicting stock prices using PCA, you would follow these steps:\n",
    "\n",
    "1. Understand the dataset: Begin by gaining a clear understanding of the dataset, including the available features such as company financial data (e.g., revenue, earnings, etc.) and market trends (e.g., interest rates, inflation, etc.).\n",
    "\n",
    "2. Preprocess the data: Ensure that the dataset is preprocessed appropriately. This may involve handling missing values, normalizing or standardizing the numerical features, and encoding categorical variables if necessary.\n",
    "\n",
    "3. Perform PCA: Apply the PCA technique to the dataset to reduce its dimensionality while preserving the most important patterns and variations in the data. Follow these steps:\n",
    "\n",
    "   a. Standardize the data: PCA is sensitive to the scale of the features, so it's important to standardize the dataset by subtracting the mean and dividing by the standard deviation of each feature.\n",
    "\n",
    "   b. Compute the covariance matrix: Calculate the covariance matrix based on the standardized dataset. The covariance matrix captures the relationships and variances between the features.\n",
    "\n",
    "   c. Calculate the eigenvectors and eigenvalues: Compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues quantify the amount of variance explained by each principal component.\n",
    "\n",
    "   d. Select the principal components: Rank the eigenvectors based on their corresponding eigenvalues. Choose the top-k eigenvectors that account for a significant amount of the total variance in the data. Typically, you would consider the principal components with the highest eigenvalues.\n",
    "\n",
    "   e. Transform the data: Multiply the standardized dataset by the selected eigenvectors to obtain the reduced set of features. This transformation represents the dataset in the lower-dimensional space spanned by the principal components.\n",
    "\n",
    "4. Determine the number of principal components: Decide on the number of principal components to retain based on the desired level of dimensionality reduction. You can consider factors such as the cumulative explained variance and domain knowledge to make an informed decision.\n",
    "\n",
    "5. Evaluate the reduced dataset: Assess the performance of your model using the reduced dataset. Compare it with the performance using the original dataset to understand the impact of dimensionality reduction.\n",
    "\n",
    "It's important to note that PCA is typically applied to numerical features rather than categorical features. If your dataset includes categorical features, additional preprocessing steps may be required, such as one-hot encoding or feature engineering techniques.\n",
    "\n",
    "By applying PCA, you can reduce the dimensionality of the dataset, eliminate redundant or less informative features, and focus on the principal components that capture the most significant information. This can help improve the model's efficiency, mitigate the curse of dimensionality, and potentially enhance prediction accuracy for stock price forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 : For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform([[1, 5, 10, 15, 20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 : For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on the dataset [height, weight, age, gender, blood pressure], the number of principal components to retain depends on the specific dataset and the desired level of dimensionality reduction. Several factors should be considered in determining the number of principal components to retain:\n",
    "\n",
    "1. Cumulative explained variance: The cumulative explained variance provides insights into the amount of information retained by each principal component. It is useful for understanding how much of the original variance in the data is captured by the selected components. A common approach is to set a threshold (e.g., 90% or 95% cumulative explained variance) and choose the number of principal components that surpass that threshold.\n",
    "\n",
    "2. Domain knowledge: Consider the domain knowledge and the importance of each feature in the context of the problem you are trying to solve. If there are specific features that are known to have a significant impact or are crucial for the problem at hand, you may want to retain principal components that capture most of the variation in those features.\n",
    "\n",
    "3. Dimensionality reduction goals: Consider the desired level of dimensionality reduction. If the original dataset has a large number of features and computational efficiency is a concern, choosing fewer principal components can help reduce the computational burden while retaining essential information. However, it's essential to strike a balance between dimensionality reduction and the potential loss of information.\n",
    "\n",
    "It is difficult to determine the exact number of principal components to retain without further information about the dataset and the specific problem. However, a common approach is to initially perform PCA and analyze the scree plot, which shows the eigenvalues of the principal components. The scree plot helps visualize the amount of variance explained by each component and can assist in determining the appropriate number of components to retain.\n",
    "\n",
    "By examining the scree plot and considering the cumulative explained variance, domain knowledge, and dimensionality reduction goals, you can make an informed decision about the number of principal components to retain for the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "72b2382ece9768098284d92bbc69d35954e75b60d1e25897d1389c232f4796f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
