{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Simple linear regression we have one independent(x) and one deependent feature (Y) and In Multiple linear regression we have more than one independent feature (xi) and one dependent feature (Y).\n",
    "#### Let's take an example for SLR: We are comparing our salary with our experience. I gave my salary and experience data to my model once my model is created. Here experience is my independent and salary is dependent feature. Once my model is trained with this data if i gave any new experience value then it must give us salary based on train data.\n",
    "#### For multiple linear regression we have car dataset, Where we want the resell value of used car. We have car_engine, car_horsepower, car_price, car_top_speed, car_purchase_date so this all are my independent feature and resell value is my dependent variable which depend on all this features. I will train my model with this data once model is trained and if i give all independent values to my model the it will give me resell value of the used cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are five assumptions of linear regression :\n",
    "#### 1. Linear Relation : We can create scatter plot to know linear relationship.\n",
    "#### 2. Multicollinearity : If we have one or more independent variable with relationship to dependent variable if one variable change then another two varible should not affect or there is no relationship between independent variable.\n",
    "#### 3. Normal Residual : We can create KDE plot or QQ plot, to check if residual is normally distributed. \n",
    "#### 4. Homoscedasticity : If we create scatter plot then spread should be same.\n",
    "#### 5. No Autocorrelation of Error : We can create line plot of residual if this plot does not give any specific pattern then it should be good otherwise not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In a linear regression model, the slope and intercept are fundamental components of the equation that describes the relationship between two variables. Here's how you interpret them:\n",
    "\n",
    "#### Intercept (b₀): The intercept, often denoted as \"b₀\" in the linear regression equation, represents the predicted value of the dependent variable when the independent variable is equal to zero. In other words, it is the y-intercept of the regression line. The intercept can have a meaningful interpretation depending on the context of your data, but in many cases, it may not have a practical meaning. Its primary purpose is to adjust the baseline or constant value of the dependent variable.\n",
    "\n",
    "#### Example: If you're analyzing a linear regression model that predicts the cost of a product based on advertising spending, the intercept could represent the estimated cost when there is no advertising spending, even though this might not be a meaningful real-world scenario.\n",
    "\n",
    "#### Slope (b₁): The slope, often denoted as \"b₁\" in the linear regression equation, represents the change in the dependent variable for a one-unit change in the independent variable. It quantifies the relationship or impact of the independent variable on the dependent variable.\n",
    "\n",
    "#### Example: If you're analyzing a linear regression model that predicts a person's weight (dependent variable) based on their height in inches (independent variable) and the slope is 3, it means that, on average, for every additional inch in height, the weight is expected to increase by 3 pounds. The slope indicates the direction and strength of the relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent is an optimization technique used in machine learning and various other fields to find the minimum of a function, typically a cost or loss function, by iteratively adjusting the model's parameters. It's a fundamental algorithm for training machine learning models, especially for models like linear regression, logistic regression, neural networks, and many other algorithms.\n",
    "\n",
    "#### The key idea behind gradient descent is to update the model's parameters (weights and biases) in the direction of the steepest decrease (negative gradient) of the cost function to minimize it. The algorithm operates as follows:\n",
    "\n",
    "#### Initialization: Start with an initial set of model parameters (weights and biases). These can be initialized randomly or with some predefined values.\n",
    "\n",
    "#### Compute the Gradient: Calculate the gradient (derivative) of the cost function with respect to each parameter. The gradient tells us the direction and magnitude of the steepest increase in the cost function.\n",
    "\n",
    "#### Update Parameters: Adjust the model parameters by moving in the opposite direction of the gradient. This is done to minimize the cost function. The update rule for each parameter is as follows:\n",
    "\n",
    "#### New Parameter = Old Parameter − Learning Rate × Gradient\n",
    "\n",
    "#### Learning Rate (α): This is a hyperparameter that determines the step size during each iteration. It controls how much the parameters are adjusted. A smaller learning rate leads to more precise but slower convergence, while a larger learning rate can lead to faster convergence but may overshoot the minimum.\n",
    "\n",
    "#### Repeat: Steps 2 and 3 are repeated for a specified number of iterations (epochs) or until the algorithm converges to a minimum. Convergence is usually determined based on some stopping criteria, such as a small change in the cost function or a fixed number of iterations.\n",
    "\n",
    "#### Gradient descent can be used in different variations, such as:\n",
    "\n",
    "#### Batch Gradient Descent: This is the standard form of gradient descent. It computes the gradient using the entire dataset at each iteration. It provides accurate but slower convergence, making it suitable for smaller datasets.\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD): In SGD, the gradient is computed for a single randomly selected data point at each iteration. It is faster but can have more fluctuation in the parameter updates.\n",
    "\n",
    "#### Mini-Batch Gradient Descent: Mini-batch gradient descent strikes a balance between batch and stochastic gradient descent. It uses a small random subset (mini-batch) of the data to compute the gradient. This is the most commonly used variation in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Linear Regression Model is extends of simple linear regression.\n",
    "#### Multiple linear regression shows relationship between one dependent feature to many independent features, So our dimension of data can be 3d, 4d or nd.\n",
    "#### We can not represent this relationship in 2 dimension graph because we have more than on independent feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multicollinearity is one of the assumption of simple linear regression. Where we try to find if there are three independent variable do they have any relationship with any other independet feature.\n",
    "#### We can use correlation matrix to find is there is any multicollinearity in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial regression is a type of regression analysis used to model the relationship between a dependent variable and an independent variable(s) when the relationship is not linear but follows a polynomial curve. Unlike simple linear regression, which models a linear relationship, polynomial regression can capture more complex and nonlinear patterns in the data. Here are the key features and differences between polynomial regression and linear regression:\n",
    "\n",
    "#### Differences:\n",
    "\n",
    "#### Model Complexity: Linear regression is simpler and models linear relationships using a straight line, while polynomial regression is more complex and accommodates curves and nonlinear patterns by adding higher-order terms to the equation.\n",
    "\n",
    "#### Linearity: Linear regression strictly assumes linearity, while polynomial regression relaxes this assumption to capture nonlinear relationships.\n",
    "\n",
    "#### Equation: Linear regression has a simple equation with a constant slope and intercept, while polynomial regression introduces multiple terms with different powers of the independent variable, allowing for more flexible modeling.\n",
    "\n",
    "#### Use Cases: Linear regression is appropriate when the relationship is truly linear, while polynomial regression is used when there is evidence of curvature or nonlinearity in the data.\n",
    "\n",
    "#### It's important to note that while polynomial regression can model complex patterns, including overfitting can be a concern. You should be cautious when selecting the degree of the polynomial (the highest power of X) to avoid overfitting the model to the noise in the data. Proper model evaluation techniques and cross-validation are essential when working with polynomial regression to ensure the model's robustness and generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial regression offers advantages and disadvantages compared to linear regression, and the choice between the two depends on the nature of the data and the underlying relationship between variables. Here are the advantages and disadvantages of polynomial regression and situations in which it is preferred:\n",
    "\n",
    "#### **Advantages of Polynomial Regression**:\n",
    "\n",
    "#### 1. **Captures Nonlinearity**: Polynomial regression can model complex, nonlinear relationships in the data. This makes it more flexible than linear regression, which assumes linearity.\n",
    "\n",
    "#### 2. **Higher Fit Accuracy**: By introducing higher-order terms (e.g., \\(X^2\\), \\(X^3\\)), polynomial regression can fit the data more closely, leading to a potentially better fit and reduced residual errors.\n",
    "\n",
    "#### 3. **Better Data Explanation**: In situations where a linear model doesn't adequately explain the data, polynomial regression can provide a more accurate description of the relationship, especially when there are curves, peaks, valleys, or other nonlinear patterns.\n",
    "\n",
    "#### **Disadvantages of Polynomial Regression**:\n",
    "\n",
    "#### 1. **Overfitting**: The increased complexity of polynomial regression models can lead to overfitting. High-degree polynomials may capture noise in the data and not generalize well to new, unseen data.\n",
    "\n",
    "#### 2. **Interpretability**: Polynomial regression models can become less interpretable as the degree of the polynomial increases. It can be challenging to understand the practical meaning of higher-order coefficients.\n",
    "\n",
    "#### 3. **Data Requirement**: Polynomial regression may require a larger dataset to estimate coefficients accurately, especially with higher-degree polynomials.\n",
    "\n",
    "#### **When to Use Polynomial Regression**:\n",
    "\n",
    "#### Polynomial regression is preferred in the following situations:\n",
    "\n",
    "#### 1. **Nonlinear Relationships**: When the relationship between the independent and dependent variables is clearly nonlinear. For example, when analyzing the effect of temperature on the growth of a plant, a simple linear model may not capture the temperature's impact correctly, as growth might initially increase and then decrease with temperature.\n",
    "\n",
    "#### 2. **Capturing Curves and Patterns**: When the data exhibits curves, peaks, valleys, or other intricate patterns. Polynomial regression allows you to fit a curve to such data more accurately.\n",
    "\n",
    "#### 3. **Exploratory Analysis**: In exploratory data analysis, polynomial regression can be used to understand the data better. It helps identify nonlinear trends that may not be evident with linear regression.\n",
    "\n",
    "#### 4. **Interpolation**: When you need to interpolate between data points, polynomial regression can provide a flexible way to estimate values between observed data points.\n",
    "\n",
    "#### 5. **When Other Models Fail**: In cases where simpler models like linear regression or other techniques do not provide an adequate fit or predictive accuracy, polynomial regression can be a useful alternative.\n",
    "\n",
    "#### It's important to note that the choice of the degree of the polynomial (e.g., quadratic, cubic) should be made carefully. Too high a degree can lead to overfitting, while too low a degree may not capture the underlying relationship. Cross-validation and model evaluation techniques should be employed to determine the optimal degree and ensure the model's generalization to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "72b2382ece9768098284d92bbc69d35954e75b60d1e25897d1389c232f4796f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
