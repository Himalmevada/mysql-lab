{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f74030-21d0-4ccf-805c-88b703545f78",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e50450-e811-405b-9882-4072608a2ed5",
   "metadata": {},
   "source": [
    "This problem requires the application of conditional probability based on the information provided.\n",
    "\n",
    "Given:\n",
    "- Probability that an employee uses the company's health insurance plan: \\( P(\\text{Uses insurance}) = 0.70 \\)\n",
    "- Probability that an employee who uses the plan is a smoker: \\( P(\\text{Smoker | Uses insurance}) = 0.40 \\)\n",
    "\n",
    "We need to find the probability that an employee is a smoker given that they use the health insurance plan, which can be represented as \\( P(\\text{Smoker | Uses insurance}) \\).\n",
    "\n",
    "We can use Bayes' theorem to find this probability:\n",
    "\n",
    "\\[ P(\\text{Smoker | Uses insurance}) = \\frac{P(\\text{Uses insurance | Smoker}) \\times P(\\text{Smoker})}{P(\\text{Uses insurance})} \\]\n",
    "\n",
    "From the given information:\n",
    "- \\( P(\\text{Uses insurance | Smoker}) \\) is not directly provided.\n",
    "\n",
    "However, we can use the fact that the conditional probability of A given B is equal to the probability of both A and B occurring divided by the probability of B occurring:\n",
    "\n",
    "\\[ P(\\text{Uses insurance | Smoker}) = \\frac{P(\\text{Smoker | Uses insurance}) \\times P(\\text{Uses insurance})}{P(\\text{Smoker})} \\]\n",
    "\n",
    "Now let's solve for \\( P(\\text{Smoker}) \\):\n",
    "- \\( P(\\text{Smoker}) \\) can be found using the complement rule:\n",
    "  \\( P(\\text{Smoker}) = 1 - P(\\text{Non-smoker}) \\)\n",
    "\n",
    "Given that \\( P(\\text{Uses insurance}) = 0.70 \\) and \\( P(\\text{Smoker | Uses insurance}) = 0.40 \\), let's calculate \\( P(\\text{Smoker}) \\) and then \\( P(\\text{Smoker | Uses insurance}) \\):\n",
    "\n",
    "\\[ P(\\text{Smoker}) = 1 - P(\\text{Non-smoker}) = 1 - (1 - P(\\text{Smoker | Uses insurance})) = P(\\text{Smoker | Uses insurance}) \\]\n",
    "\\[ P(\\text{Smoker}) = 0.40 \\]\n",
    "\n",
    "Now, we can substitute the values into the formula for \\( P(\\text{Smoker | Uses insurance}) \\):\n",
    "\n",
    "\\[ P(\\text{Smoker | Uses insurance}) = \\frac{P(\\text{Smoker | Uses insurance}) \\times P(\\text{Uses insurance})}{P(\\text{Smoker})} \\]\n",
    "\\[ P(\\text{Smoker | Uses insurance}) = \\frac{0.40 \\times 0.70}{0.40} = 0.70 \\]\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is \\( 0.70 \\) or \\( 70\\% \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb5feb-c56b-457a-873f-3512def8018f",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57994dfc-ad7f-4ad9-bdb9-9f4d92d00763",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm that are commonly used for text classification, document categorization, and other tasks involving discrete features. The main difference between them lies in how they model and handle the input features.\n",
    "\n",
    "### Bernoulli Naive Bayes:\n",
    "\n",
    "1. **Feature Representation:**\n",
    "   - Assumes that input features are binary (presence or absence).\n",
    "   - Typically used for binary or boolean features, such as word presence in text.\n",
    "\n",
    "2. **Data Type:**\n",
    "   - Well-suited for binary data, like document classification where the presence or absence of words is considered.\n",
    "\n",
    "3. **Probability Calculation:**\n",
    "   - Computes the probability of each feature being present in a document and ignores the frequency of the feature.\n",
    "   - Ignores the number of occurrences of features within documents.\n",
    "\n",
    "### Multinomial Naive Bayes:\n",
    "\n",
    "1. **Feature Representation:**\n",
    "   - Assumes that input features are discrete and represent counts or frequencies (e.g., word counts in text).\n",
    "   - Suitable for text classification where the frequency of words matters.\n",
    "\n",
    "2. **Data Type:**\n",
    "   - Commonly used for text classification tasks where features are counts (e.g., word counts in a document).\n",
    "\n",
    "3. **Probability Calculation:**\n",
    "   - Considers the frequency of each feature in a document.\n",
    "   - Takes into account the number of occurrences of features within documents.\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Bernoulli Naive Bayes:**\n",
    "  - Often used in spam filtering where features are binary (presence or absence of certain words).\n",
    "  - Document classification tasks where the focus is on whether specific words are present in a document.\n",
    "\n",
    "- **Multinomial Naive Bayes:**\n",
    "  - Commonly employed in natural language processing tasks like text categorization.\n",
    "  - Document classification where the frequency of words is important in determining the document's category.\n",
    "\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the data and how the features are represented. If the features are binary, Bernoulli Naive Bayes is more appropriate, while Multinomial Naive Bayes is suitable for scenarios where features are discrete and represent counts or frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe936927-36fb-438c-adff-055ae01e50ae",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1b228-cabb-4b62-a63e-2035d5e1b994",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes, like other Naive Bayes variants, generally assumes that missing values are explicitly represented in the dataset as a specific category or feature value. In the case of Bernoulli Naive Bayes, which deals with binary features (presence or absence), handling missing values depends on how the missing values are treated or encoded in the dataset.\n",
    "\n",
    "Here are a few common approaches to handle missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "### Imputation or Encoding Missing Values:\n",
    "\n",
    "1. **Introduce a Specific Category:**\n",
    "   - Encode missing values as a distinct category or value.\n",
    "   - Treat missing values as another state of the feature, considering them separately from the presence or absence of the feature.\n",
    "   - During training, the model learns to distinguish this specific category as part of the feature set.\n",
    "\n",
    "2. **Impute with the Most Frequent Value:**\n",
    "   - Replace missing values with the most frequent value (either '0' or '1') based on the frequency of that feature in the dataset.\n",
    "   - This method assumes that the missing values are more likely to align with the prevalent state of the feature.\n",
    "\n",
    "3. **Use Missing Indicator Approach:**\n",
    "   - Create an additional binary feature indicating whether the original feature was missing or not.\n",
    "   - Treat the absence of the feature as a separate binary indicator, which might provide useful information to the classifier.\n",
    "\n",
    "### Impact on Model Performance:\n",
    "\n",
    "Handling missing values appropriately is crucial as it can affect the model's performance. The method chosen to handle missing values might influence how the model learns from the data and the resulting predictions.\n",
    "\n",
    "In Bernoulli Naive Bayes, the model typically treats missing values as a separate category or incorporates the most frequent value as a substitute. However, the specific approach to handling missing values might vary based on the nature of the dataset and the problem at hand.\n",
    "\n",
    "It's important to preprocess the data carefully, considering the implications of missing values on the model's performance and the accuracy of the predictions. Experimentation with different approaches and evaluating the impact on model performance through cross-validation or other validation methods is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48371ff-0b29-44c3-b25f-0cf393d559d7",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b7c7e7-3ac7-402a-b2e5-ba8d5be20549",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification problems. While it is commonly associated with binary or two-class classification due to its simplicity and assumptions regarding normally distributed continuous features, it can also handle multiple classes through various strategies.\n",
    "\n",
    "### Strategy for Multi-class Classification using Gaussian Naive Bayes:\n",
    "\n",
    "1. **One-vs-Rest (OvR) Strategy:**\n",
    "   - Transform the multi-class problem into multiple binary classification sub-problems.\n",
    "   - Train a separate Gaussian Naive Bayes classifier for each class against all other classes (hence, \"one-vs-rest\").\n",
    "   - For each class, the model distinguishes that class from all other classes, effectively creating a binary classifier for each class.\n",
    "   - During prediction, the class with the highest probability output from the individual classifiers is assigned to the input sample.\n",
    "\n",
    "2. **One-vs-One (OvO) Strategy:**\n",
    "   - Create binary classifiers for each pair of classes (hence, \"one-vs-one\").\n",
    "   - Train a Gaussian Naive Bayes classifier for every pair of classes.\n",
    "   - During prediction, each classifier votes for a class, and the class with the most votes is assigned to the input sample.\n",
    "\n",
    "### Advantages and Considerations:\n",
    "\n",
    "- **Simplicity:** Gaussian Naive Bayes is computationally efficient and simple, making it suitable for multi-class problems when combined with OvR or OvO strategies.\n",
    "- **Assumption of Normality:** Gaussian Naive Bayes assumes features are normally distributed within each class. Therefore, it might not perform optimally if this assumption is violated.\n",
    "\n",
    "### Implementation in Libraries:\n",
    "\n",
    "Popular machine learning libraries like scikit-learn in Python provide implementations of Gaussian Naive Bayes that can handle multi-class classification using the OvR or OvO strategies.\n",
    "\n",
    "While Gaussian Naive Bayes is not the most sophisticated classifier, it can serve as a good baseline model for multi-class classification tasks, especially when the assumption of normally distributed features holds reasonably well within each class. However, in cases where this assumption doesn't hold, other classifiers might be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781deb5e-ef0a-4a1c-a290-20d54bb4e01e",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "- Data preparation:\n",
    "    - Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "- Implementation:\n",
    "    - Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "- Results:\n",
    "    - Report the following performance metrics for each classifier:\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1 score\n",
    "- Discussion:\n",
    "    - Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "- Conclusion:\n",
    "    - Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b1597122-5b40-4b3c-9240-92f879ff104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17e9cb8b-f104-49f5-b141-42e3f8eec108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "spambase = fetch_ucirepo(id=94) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = spambase.data.features \n",
    "y = spambase.data.targets \n",
    "  \n",
    "# metadata \n",
    "# print(spambase.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(spambase.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2d2ed0cd-95a3-4091-b12c-0f0d243ee56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df801a76-d426-4fc3-8ae0-b5d5d63f4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "91249a7a-b93c-49f1-90c1-05d33fbf1bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"Gaussian NB\" : GaussianNB(), \"Bernoulli NB\" : BernoulliNB(), \"Multinomial NB\" : MultinomialNB()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "84d92058-b417-4c2c-a26b-32ee0229d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {\"var_smoothing\":[1e-9]},\n",
    "    {\"alpha\":[1.0,0.1,0.01,0.001]},\n",
    "    {\"alpha\":[1.0,0.1,0.01,0.001]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0010e3bc-d30e-4b81-a722-e7e320010eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gaussian NB': 0.8208469055374593, 'Bernoulli NB': 0.8794788273615635, 'Multinomial NB': 0.7871878393051032}\n",
      "{'Gaussian NB': 0.7288135593220338, 'Bernoulli NB': 0.9378531073446328, 'Multinomial NB': 0.8380414312617702}\n",
      "{'Gaussian NB': 0.9485294117647058, 'Bernoulli NB': 0.8645833333333334, 'Multinomial NB': 0.8018018018018018}\n",
      "{'Gaussian NB': 2.0, 'Bernoulli NB': 2.0, 'Multinomial NB': 2.0}\n"
     ]
    }
   ],
   "source": [
    "accuracy = {}\n",
    "precision = {}\n",
    "recall = {}\n",
    "f1_score = {}\n",
    "\n",
    "for i in range(len(models)):\n",
    "    \n",
    "    model = list(models.values())[i]\n",
    "    \n",
    "    clf = GridSearchCV(estimator=model,param_grid=params[i],cv=10)\n",
    "    \n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    cn_matrix = confusion_matrix(y_test,y_pred)\n",
    "    tp = cn_matrix[0][0]\n",
    "    fp = cn_matrix[0][1]\n",
    "    tn = cn_matrix[1][1]\n",
    "    fn = cn_matrix[1][0]\n",
    "    \n",
    "    accuracy_score = (tp + tn) / (tp + fp + tn + fn)\n",
    "    precision_score = tp / (tp + fp)\n",
    "    recall_score = tp / (tp + fn)\n",
    "    f1score = 2 * ((precision_score * recall_score) / (precision_score * recall_score))\n",
    "    \n",
    "    accuracy[list(models.keys())[i]] = accuracy_score\n",
    "    precision[list(models.keys())[i]] = precision_score\n",
    "    recall[list(models.keys())[i]] = recall_score\n",
    "    f1_score[list(models.keys())[i]] = f1score\n",
    "    \n",
    "else:\n",
    "    print(accuracy)\n",
    "    print(precision)\n",
    "    print(recall)\n",
    "    print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f72637-9387-4b50-bab8-acf08363bca9",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes gives us almost 95% accuracy and Bernoulli Naive Bayes gives us almost 94% for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79d58d-f38e-4d8f-9a73-73de7905acf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
