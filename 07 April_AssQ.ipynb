{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca00a99-1a88-4b03-8db4-4ea3ec224e28",
   "metadata": {},
   "source": [
    "### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1228c-b508-460e-9b48-36491d1384ee",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions share a relationship in machine learning, particularly in algorithms like Support Vector Machines (SVMs) where kernel methods are employed for non-linear transformations of data. \n",
    "\n",
    "### Polynomial Functions:\n",
    "- Polynomial functions are mathematical functions of the form \\( f(x) = a_nx^n + a_{n-1}x^{n-1} + ... + a_1x + a_0 \\), where \\( x \\) is the variable, \\( a_n \\) are coefficients, and \\( n \\) is the degree.\n",
    "- In the context of machine learning, polynomial functions are used for feature transformation. For instance, in polynomial regression, a polynomial function is used to model the relationship between the features and the target variable by introducing polynomial terms (e.g., \\( x^2 \\), \\( x^3 \\)) of the original features.\n",
    "\n",
    "### Kernel Functions:\n",
    "- Kernel functions in machine learning define similarity measures between data points in a higher-dimensional space without explicitly transforming the data into that space.\n",
    "- Polynomial kernel functions are one type of kernel function used in SVMs and other algorithms. They compute the similarity (or inner product) between two data points as if they were mapped into a higher-dimensional space using polynomial transformations.\n",
    "\n",
    "### Relationship:\n",
    "- Polynomial functions are used to perform explicit transformations on features to a higher-dimensional space.\n",
    "- Polynomial kernel functions, on the other hand, implicitly compute the similarity between data points as if they were transformed into a higher-dimensional space using polynomial transformations, without actually performing the transformation explicitly.\n",
    "\n",
    "### Polynomial Kernel:\n",
    "- The polynomial kernel function is represented as \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)^d \\), where \\( \\mathbf{x}_i \\) and \\( \\mathbf{x}_j \\) are data points, \\( \\gamma \\) is a coefficient, \\( r \\) is an optional constant term, and \\( d \\) is the degree of the polynomial.\n",
    "- It computes the dot product of the transformed feature vectors without explicitly performing the transformation, thereby allowing SVMs to learn non-linear decision boundaries using polynomial functions.\n",
    "\n",
    "### Summary:\n",
    "- Polynomial functions are used explicitly to transform features to a higher-dimensional space.\n",
    "- Polynomial kernel functions in algorithms like SVMs perform similar transformations implicitly by computing similarities between data points as if they were transformed into a higher-dimensional space using polynomial functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c71184-7055-4b06-86de-9221a95d53c7",
   "metadata": {},
   "source": [
    "### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38923324-9dc2-4769-b129-03c30492c696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generating a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Splitting the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating an SVM model with a polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3)  # You can adjust the degree as needed\n",
    "\n",
    "# Training the SVM model\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d46aa61-ef83-4dca-bc27-5914be40bd8f",
   "metadata": {},
   "source": [
    "### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3af75-1d60-4e2a-b1de-b0ca56a046e2",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), epsilon (\\(\\varepsilon\\)) is a hyperparameter that determines the width of the margin of tolerance around the predicted values (known as the epsilon-tube). It influences the trade-off between the model's complexity and its ability to capture deviations within a certain margin.\n",
    "\n",
    "### Relationship between Epsilon and Support Vectors:\n",
    "\n",
    "1. **Larger Epsilon:** When the value of epsilon is increased, the margin of tolerance around the predicted values becomes wider. This means the model allows for larger deviations between the predicted and actual target values.\n",
    "\n",
    "2. **Impact on Support Vectors:** Increasing epsilon typically results in fewer support vectors. A larger epsilon allows more data points to fall within the margin of tolerance, reducing the need for additional support vectors to define the regression function within the margin.\n",
    "\n",
    "3. **Smoother Predictions:** With a larger epsilon, the model tends to have smoother predictions, as it tolerates larger errors and is less sensitive to individual data points. This can lead to a simpler model with fewer support vectors.\n",
    "\n",
    "### Summary:\n",
    "- **Larger epsilon:** \n",
    "  - Wider margin of tolerance.\n",
    "  - Fewer support vectors as the model tolerates larger deviations.\n",
    "  - Smoother predictions, potentially resulting in a simpler model.\n",
    "\n",
    "It's essential to carefully select the value of epsilon based on the problem's requirements, balancing the need for model flexibility with the goal of avoiding overfitting or underfitting. Cross-validation or grid search can help identify an appropriate epsilon value that optimizes model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc02cd-5c59-465f-8029-a7bb084907d2",
   "metadata": {},
   "source": [
    "### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e0147-e751-4788-a322-14b2c7371dff",
   "metadata": {},
   "source": [
    "Absolutely, each parameter in Support Vector Regression (SVR) plays a crucial role in determining the model's performance and its ability to generalize to unseen data. Let's break down the impact and functionality of each parameter:\n",
    "\n",
    "### 1. Kernel Function:\n",
    "- **Function:** Determines the type of mapping used to transform the input features into a higher-dimensional space.\n",
    "- **Effect:** Affects the model's ability to capture non-linear patterns in the data.\n",
    "- **Examples:**\n",
    "  - Use a `linear` kernel for linear relationships.\n",
    "  - Choose `poly` for polynomial relationships.\n",
    "  - Opt for `rbf` (Radial Basis Function) for complex, non-linear patterns.\n",
    "\n",
    "### 2. C Parameter:\n",
    "- **Function:** Controls the trade-off between minimizing the error and maximizing the margin.\n",
    "- **Effect:** Regulates the penalty for errors, influencing the model's flexibility.\n",
    "- **Examples:**\n",
    "  - Increase `C` to reduce the margin of tolerance, potentially leading to a model that fits the training data more closely.\n",
    "  - Decrease `C` for a wider margin and more tolerance for errors, promoting a smoother model.\n",
    "\n",
    "### 3. Epsilon Parameter (\\(\\varepsilon\\)):\n",
    "- **Function:** Specifies the margin of tolerance around the predicted values.\n",
    "- **Effect:** Governs the width of the epsilon-tube around the regression line.\n",
    "- **Examples:**\n",
    "  - Increase \\(\\varepsilon\\) to allow larger deviations between predicted and actual values, resulting in fewer support vectors.\n",
    "  - Decrease \\(\\varepsilon\\) for a narrower margin, potentially leading to more support vectors and a tighter fit.\n",
    "\n",
    "### 4. Gamma Parameter:\n",
    "- **Function:** Defines the influence of a single training example, affecting the curvature of the decision boundary.\n",
    "- **Effect:** Controls the reach of the kernel function and the impact of individual data points.\n",
    "- **Examples:**\n",
    "  - Increase `gamma` to consider only nearby points for modeling, creating a more complex and sensitive model.\n",
    "  - Decrease `gamma` to have a broader influence, potentially leading to smoother predictions.\n",
    "\n",
    "### Parameter Choices:\n",
    "- **Balancing Act:** These parameters are interconnected and involve a trade-off between model complexity and generalization.\n",
    "- **Tuning:** Parameter values are typically optimized using techniques like grid search or randomized search, considering the dataset and problem requirements.\n",
    "- **Overfitting vs. Underfitting:** Be mindful of overfitting (high model complexity) or underfitting (oversimplified model) while adjusting these parameters.\n",
    "\n",
    "### Example Scenarios:\n",
    "- **High Variance:** Increase `C` and decrease \\(\\varepsilon\\) to reduce tolerance for errors.\n",
    "- **Overfitting:** Decrease `gamma` to have a smoother decision boundary.\n",
    "- **Non-Linearity:** Choose a suitable kernel function (`poly` or `rbf`) to capture complex relationships.\n",
    "\n",
    "In practice, finding the right combination of these parameters involves experimentation, domain knowledge, and understanding the dataset's characteristics to achieve optimal SVR performance. Tuning these parameters is crucial for obtaining a well-performing and well-generalizing model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf92d58-2b1a-4e0e-b8c2-b602d8e2e459",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataset.\n",
    "- Split the dataset into training and testing set.\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normalization).\n",
    "- Create an instance of the SVC classifier and train it on the training data.\n",
    "- hse the trained classifier to predict the labels of the testing data.\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-score.\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance.\n",
    "- Train the tuned classifier on the entire dataset.\n",
    "- Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "18d7b0f7-8f6d-4be2-8459-85855716b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "import pickle\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9ad713df-321a-43b6-88a8-47013ff7bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ea5c2006-6f2e-4dc7-b403-ccb4f9ca1165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "331114e9-ca67-4974-810a-e581e63161e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=dataset.data,columns=dataset.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e8a61e05-fae4-403a-869f-0f29b7b01962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "be5ee8b3-796e-48bc-b474-fd28f4db42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0b95c871-853a-44e9-996f-603df670b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e6848942-1543-49fe-a7c0-19f563e5106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b809ea10-35d2-47c1-a80d-58bc757caf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "77775bec-369f-48b6-908c-5f65f39f0c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "645b2771-a09a-4245-8a63-1ed29ad1b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "aac9569c-36a4-4e67-8794-27f1182b1409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  0 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e42f87d2-494a-4ba1-9390-1d6b54ce97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'C': [0.1, 1, 10, 100],  # Values for the C parameter\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],  # Values for the gamma parameter\n",
    "    'kernel': ['linear', 'rbf', 'poly']  # Kernel functions to try\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c2ab1007-5767-4f6d-a543-aae4203d0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_cv = GridSearchCV(classifier,param_grid=params,cv=5,scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "62f686ce-2f1f-4fd9-bb7d-155c670127ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1],\n",
       "                         'kernel': ['linear', 'rbf', 'poly']},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "609c9287-f922-4583-a4f4-3a9eda8f1b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 0.1, 'kernel': 'poly'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a6a4c52a-956f-4c7c-a562-00ec1620bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = class_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3af2efae-cfe9-4a68-9a80-0dc56d05f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  3  9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       0.79      1.00      0.88        11\n",
      "           2       1.00      0.75      0.86        12\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.93      0.92      0.91        38\n",
      "weighted avg       0.94      0.92      0.92        38\n",
      "\n",
      "0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d1fe879c-2998-4db1-a51c-7785d3927162",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(\"SVC.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "28eee228-2997-4000-bf83-5a260acac101",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_algo = pickle.load(open(\"SVC.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dccd35c5-ffa8-4b1c-bfc2-1848379e037b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_algo.predict([[5.1,3.5,1.4]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
