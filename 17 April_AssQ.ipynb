{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7fba4df-62fe-41e8-81e3-3699bcf0f9cb",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6805374a-eddb-4f2a-bd8c-24d29de5911b",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique that's used for regression tasks, aiming to predict continuous numerical values. It's an extension of the gradient boosting framework but tailored for regression problems.\n",
    "\n",
    "Here's an overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Base Model (Weak Learner):** Similar to other boosting techniques, Gradient Boosting Regression starts with an initial model, often a simple decision tree with limited depth (a decision stump). This serves as the first weak learner.\n",
    "\n",
    "2. **Sequential Training:** Unlike simultaneous training of models in parallel (as in random forests), Gradient Boosting Regression sequentially trains a series of weak learners. Each subsequent weak learner focuses on the errors or residuals made by the combination of the existing weak learners.\n",
    "\n",
    "3. **Minimization of Loss Function:** The algorithm minimizes a loss function (usually a differentiable loss function like squared error loss for regression problems) by iteratively fitting new models to the residuals of the previous predictions.\n",
    "\n",
    "4. **Gradient Descent Optimization:** Gradient Boosting Regression employs gradient descent optimization to minimize the loss function. It calculates the gradient of the loss function with respect to the model's prediction and adjusts the new model's parameters in the direction that minimizes this gradient.\n",
    "\n",
    "5. **Adding Weak Learners:** Weak learners are added iteratively, and each new learner focuses on the residuals or errors left by the combined predictions of the existing ensemble.\n",
    "\n",
    "6. **Combining Predictions:** The predictions from all weak learners are combined through a weighted sum to create the final prediction. The weights for each weak learner are determined based on their contribution to minimizing the overall loss function.\n",
    "\n",
    "7. **Regularization:** Gradient Boosting Regression often incorporates regularization techniques to prevent overfitting, such as controlling tree depth, adding learning rate shrinkage, or applying L1/L2 regularization.\n",
    "\n",
    "Gradient Boosting Regression is known for its ability to handle complex relationships in data and its capacity to provide highly accurate predictions for regression problems. Popular libraries like XGBoost, LightGBM, and scikit-learn's GradientBoostingRegressor implement variations of this algorithm and offer efficient implementations with additional features for optimization and performance enhancement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb49bbd-cc79-455d-936b-ce6c73d734f6",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9087433b-560e-446c-82c3-2e03ed18716c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.799515628059908\n",
      "R-squared: 0.9182856157988541\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a simple dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X[:, 0] + np.random.randn(100)  # True relationship: y = 2*X + noise\n",
    "\n",
    "# Gradient Boosting Regression implementation\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.estimators = []\n",
    "        self.intercept = np.mean(y)  # Initialize with mean of y\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        predictions = np.full(len(y), self.intercept)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Train a weak learner (decision stump)\n",
    "            tree = DecisionStump()\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update predictions with the new weak learner\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Store the weak learner\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.full(len(X), self.intercept)\n",
    "        \n",
    "        for tree in self.estimators:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Define a simple decision stump as a weak learner\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.prediction = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        best_mse = float('inf')\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature_index] < threshold\n",
    "                left_mse = np.mean((y[left_indices] - np.mean(y[left_indices])) ** 2)\n",
    "                right_mse = np.mean((y[~left_indices] - np.mean(y[~left_indices])) ** 2)\n",
    "                mse = left_mse + right_mse\n",
    "                \n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    self.feature_index = feature_index\n",
    "                    self.threshold = threshold\n",
    "                    self.prediction = np.mean(y[left_indices])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(X[:, self.feature_index] < self.threshold, self.prediction, -self.prediction)\n",
    "\n",
    "# Instantiate and train the GradientBoostingRegressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_regressor.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34401796-e685-4af4-beda-d5a1cf47a243",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacc3c88-2309-4f8e-8db7-9d7e97533539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\n",
      "Mean Squared Error: 0.001710594918236127\n",
      "R-squared: 0.999998800720068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a simple dataset for regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.2, random_state=42)\n",
    "\n",
    "# Define the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Set up hyperparameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=5, verbose=1)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Use the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate best model\n",
    "y_pred = best_model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fee8cc-89a0-42ac-a034-b6479e57a747",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7440940c-9e22-4f99-b35e-67eae3aa62f4",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner refers to a simple or relatively less complex model that performs slightly better than random chance on a given problem. These models are used as building blocks or base estimators within the boosting framework.\n",
    "\n",
    "The characteristics of a weak learner in Gradient Boosting include:\n",
    "\n",
    "1. **Limited Complexity:** Weak learners are usually simple models with limited complexity. For example, in decision tree-based boosting algorithms, weak learners are often shallow decision trees, also known as decision stumps, consisting of just a single split.\n",
    "\n",
    "2. **Low Prediction Accuracy:** Individually, weak learners might not have high accuracy or predictive power compared to more complex models. They may perform only slightly better than random guessing on the training data.\n",
    "\n",
    "3. **Emphasis on Errors:** Weak learners focus on areas where the previous models in the ensemble make mistakes. In each iteration of boosting, subsequent weak learners are trained to minimize the errors or residuals left by the ensemble of previous weak learners.\n",
    "\n",
    "4. **Contribution to Ensemble:** Although weak learners themselves might not be highly accurate, their collective contribution, when combined in an ensemble, leads to a strong model with significantly improved predictive performance.\n",
    "\n",
    "In Gradient Boosting, the iterative nature of the algorithm allows weak learners to be sequentially added to the ensemble, each one addressing the deficiencies of the combined model from previous iterations. By emphasizing the difficult-to-predict instances, these weak learners collectively contribute to the creation of a robust and accurate predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aadcd3a-1e53-4522-972d-c175275bc0f3",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd5a9f-411d-41e9-ab95-5c7a9f0954bc",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm lies in the iterative process of combining weak learners to create a strong predictive model. Here's a step-by-step intuition for how Gradient Boosting works:\n",
    "\n",
    "1. **Starting Point:** The process begins with an initial prediction, often the mean or a simple model like a decision stump, which serves as the starting point for the ensemble.\n",
    "\n",
    "2. **Sequential Improvement:** Gradient Boosting works sequentially, where each subsequent weak learner is trained to correct the errors or residuals made by the ensemble of the existing weak learners. It focuses on the instances where the current model performs poorly.\n",
    "\n",
    "3. **Gradient Descent Optimization:** The algorithm minimizes the loss function by using gradient descent optimization. It calculates the gradient of the loss function with respect to the model's predictions and adjusts the new model's parameters in the direction that reduces this gradient.\n",
    "\n",
    "4. **Iterative Learning:** Weak learners are added iteratively, and each new learner aims to minimize the errors left by the combination of the existing ensemble, placing emphasis on the misclassified or difficult-to-predict instances.\n",
    "\n",
    "5. **Weighted Combination:** The predictions from all weak learners are combined through a weighted sum or a weighted voting scheme. Each weak learner's prediction is weighted based on its contribution to minimizing the overall loss function.\n",
    "\n",
    "6. **Reduced Residuals:** With each iteration, the model's focus shifts towards reducing the residuals or errors that previous models in the ensemble couldn't capture accurately. This continual refinement gradually improves the overall predictive power of the ensemble.\n",
    "\n",
    "7. **Ensemble Synergy:** By combining multiple weak learners, each addressing different aspects of the data, Gradient Boosting creates a powerful ensemble model that learns complex relationships and achieves higher predictive accuracy than any individual weak learner.\n",
    "\n",
    "The intuition is that through this iterative process of sequentially improving weak learners, focusing on areas of previous weaknesses, and combining their predictions smartly, Gradient Boosting creates a strong ensemble model capable of making accurate predictions on complex datasets. The emphasis on sequentially correcting errors gradually builds a highly adaptable model capable of capturing intricate patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d306271-b2ca-4d4f-9c59-54f5a3252dc9",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24259b19-7b1e-4537-9d86-ebbbdf6edfdb",
   "metadata": {},
   "source": [
    "Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. Here's an overview of how this process works:\n",
    "\n",
    "1. **Initialization:** The algorithm starts with an initial prediction, often the mean or a simple model like a decision stump, which serves as the starting point for the ensemble.\n",
    "\n",
    "2. **Iterative Training:** It iterates through a series of steps to sequentially add weak learners to the ensemble. At each iteration:\n",
    "   \n",
    "   a. **Calculate Residuals:** The current ensemble's predictions are compared to the actual target values, and the residuals (errors) are computed. These residuals represent the difference between the predictions and the true values.\n",
    "   \n",
    "   b. **Train Weak Learner:** A new weak learner (e.g., decision stump) is trained on the residuals. This learner is focused on capturing the patterns or relationships that the current ensemble failed to capture adequately.\n",
    "   \n",
    "   c. **Update Ensemble:** The predictions of the new weak learner are added to the ensemble, with a scaled contribution. The ensemble now includes the newly trained weak learner's prediction, adjusted by a learning rate to control the step size in the direction of minimizing the error.\n",
    "   \n",
    "   d. **Update Residuals:** The residuals are updated using the new predictions added by the latest weak learner. The subsequent weak learner is trained on these updated residuals, emphasizing areas where the current ensemble still makes errors.\n",
    "\n",
    "3. **Sequential Improvement:** The process continues for a predefined number of iterations or until a stopping criterion is met. With each iteration, the ensemble focuses on reducing the errors or residuals left by the combination of existing weak learners.\n",
    "\n",
    "4. **Combining Predictions:** The final prediction is made by aggregating the predictions of all weak learners in the ensemble. Each weak learner's prediction is weighted by its contribution to minimizing the loss function during training.\n",
    "\n",
    "By iteratively adding weak learners and focusing on areas where the current ensemble makes mistakes, Gradient Boosting builds an ensemble that collectively corrects its errors, gradually improving its overall predictive accuracy. The ensemble becomes stronger through the synergy of multiple weak learners, each addressing different aspects of the data, resulting in a powerful and accurate predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8cba4-d18c-480e-af11-a5b7e86a6ef8",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1de7f5-5c28-426c-b772-1206d07a471a",
   "metadata": {},
   "source": [
    "Sure, let's break down the mathematical intuition behind Gradient Boosting:\n",
    "\n",
    "1. **Loss Function:** Gradient Boosting minimizes a differentiable loss function (e.g., mean squared error for regression) that measures the difference between predictions and actual values.\n",
    "\n",
    "2. **Starting Point:** Initialize the model with a simple prediction, often the mean of the target values, which serves as the initial prediction.\n",
    "\n",
    "3. **Residuals Calculation:** Compute the residuals or errors between the current predictions and the actual target values. These residuals represent the gradient of the loss function with respect to the current predictions.\n",
    "\n",
    "4. **Sequential Learning:** Iteratively build weak learners to address the residuals:\n",
    "\n",
    "    a. **Train Weak Learner:** Train a weak learner (like a decision stump) to fit the residuals. The weak learner aims to minimize the loss function (negative gradient) by fitting the errors made by the current ensemble.\n",
    "    \n",
    "    b. **Learning Rate Adjustment:** Scale the predictions of the weak learner by a learning rate to control the contribution of each weak learner to the overall ensemble.\n",
    "    \n",
    "    c. **Update Predictions:** Update the ensemble predictions by adding the scaled predictions of the new weak learner to the existing predictions.\n",
    "\n",
    "5. **Updated Residuals:** Recalculate the residuals using the updated predictions. These updated residuals represent the new gradient or errors that subsequent weak learners should focus on.\n",
    "\n",
    "6. **Iterative Refinement:** Repeat steps 4 and 5 for a predefined number of iterations or until a stopping criterion is met. Each weak learner is trained to minimize the errors left by the existing ensemble, sequentially improving the predictions.\n",
    "\n",
    "7. **Combination of Weak Learners:** Finally, combine the predictions of all weak learners in the ensemble by aggregating their contributions. This aggregation could involve a weighted sum or a weighted voting scheme based on the weak learners' performances.\n",
    "\n",
    "8. **Final Prediction:** The aggregated predictions from the ensemble of weak learners constitute the final prediction made by the Gradient Boosting algorithm.\n",
    "\n",
    "Mathematically, Gradient Boosting optimizes the ensemble by minimizing the loss function in the direction that reduces the errors or residuals made by the current model, gradually improving its predictive accuracy with each iteration. The sequential addition of weak learners focused on minimizing errors leads to the construction of a strong and accurate predictive model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
